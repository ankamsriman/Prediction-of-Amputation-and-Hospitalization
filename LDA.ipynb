{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import re\n",
    "import string\n",
    "\n",
    "import spacy\n",
    "\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "\n",
    "# libraries for visualization\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text): \n",
    "    return text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# review_data= pd.read_csv(\"C:\\\\AmazonReviews\\\\Reviews.csv\")\n",
    "df = pd.read_csv('original 2.csv', encoding='cp1252')\n",
    "df=pd.DataFrame(df.values)\n",
    "df.rename(columns = {0:'IDUPA',1:'EventDate',2:'Hospitalized',3:'Amputation',4:'Nature',5:'NatureTitle',6:'Part of Body',7:'Part of Body Title',8:'Event',9:'EventTitle',10:'Source',11:'SourceTitle'}, inplace = True)\n",
    "review_data=df\n",
    "review_data=review_data[\"Part of Body Title\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dateutil import parser\n",
    "df['time_stamp'] = df['EventDate'].map(parser.parse)\n",
    "df['event_year'] = df['time_stamp'].map(lambda x: x.year)\n",
    "df['event_month'] = df['time_stamp'].map(lambda x: x.month)\n",
    "df['event_weekday'] = df['time_stamp'].map(lambda x: x.dayofweek)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=set(df['Hospitalized'].values)\n",
    "x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hos_ind = df[ (df['Hospitalized'] != 0) & (df['Hospitalized'] != 1) ].index\n",
    "len(hos_ind)\n",
    "df.drop(hos_ind , inplace=True)\n",
    "df.head(5)\n",
    "\n",
    "x=set(df['Amputation'].values)\n",
    "x\n",
    "hos_ind = df[ (df['Amputation'] != 0) & (df['Amputation'] != 1) ].index\n",
    "len(hos_ind)\n",
    "df.drop(hos_ind , inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.Hospitalized.value_counts())\n",
    "print(df.Amputation.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# special"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Separate majority and minority classes\n",
    "# from sklearn.utils import resample\n",
    "\n",
    "# df_minority = df.loc[df.Hospitalized == 0].copy()\n",
    "# df_majority = df.loc[df.Hospitalized == 1].copy()\n",
    "# print(df_minority.shape)\n",
    "# print(df_majority.shape)\n",
    "# print(df.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(df.Hospitalized.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Upsample minority class\n",
    "# df_minority_upsampled = resample(df_minority,\n",
    "#                              replace=True,  # sample with replacement\n",
    "#                              n_samples=58097,  # to match majority class\n",
    "#                              random_state=42)  # reproducible results\n",
    "\n",
    "# # Combine majority class with upsampled minority class\n",
    "# df_upsampled = pd.concat([df_majority, df_minority_upsampled])\n",
    "\n",
    "# # Display new class counts\n",
    "# print(df_upsampled.Hospitalized.value_counts())\n",
    "# print(df_upsampled.Amputation.value_counts())\n",
    "# df=df_upsampled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(df.Hospitalized.value_counts())\n",
    "# print(df.Amputation.value_counts())\n",
    "# df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# special"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_accident_by_year(df, norm=False, save=False):\n",
    "    xtab = pd.crosstab(df.event_year, df.Hospitalized)\n",
    "    xtab.columns = ['Not Hospitalized', 'Hospitalized']\n",
    "    xtab.columns = ['0','1']\n",
    "    xtab.columns.name = 'Hospitalized Outcome'\n",
    "    ylabel = 'Hospitalized Count'\n",
    "    if norm:\n",
    "        xtab = xtab.div(xtab.sum(axis=1).astype('float'), axis=0)\n",
    "        ylabel = 'Hospitalized Percent'\n",
    "    fig, axes = plt.subplots(1, 1, figsize=(10, 5))\n",
    "    xtab.plot(kind='bar', ax=axes, stacked=True, alpha=0.5, sort_columns=True)\n",
    "    axes.set_title('OSHA Accident Investigations')\n",
    "    axes.set_ylabel(ylabel)\n",
    "    axes.set_xlabel('Event Year')\n",
    "#     if save:\n",
    "#         plt.savefig(structured_dir + 'osha_accident_by_year_plot.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plot_accident_by_year(df, save=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_accident_by_year(df, norm=False, save=False):\n",
    "    xtab = pd.crosstab(df.event_year, df.Amputation)\n",
    "    xtab.columns = ['Not Amputation', 'Amputation']\n",
    "    xtab.columns = ['0','1']\n",
    "    xtab.columns.name = 'Amputation Outcome'\n",
    "    ylabel = 'Amputation Count'\n",
    "    if norm:\n",
    "        xtab = xtab.div(xtab.sum(axis=1).astype('float'), axis=0)\n",
    "        ylabel = 'Accident Percent'\n",
    "    fig, axes = plt.subplots(1, 1, figsize=(10, 5))\n",
    "    xtab.plot(kind='bar', ax=axes, stacked=True, alpha=0.5, sort_columns=True)\n",
    "    axes.set_title('OSHA Accident Investigations')\n",
    "    axes.set_ylabel(ylabel)\n",
    "    axes.set_xlabel('Event Year')\n",
    "#     if save:\n",
    "#         plt.savefig(structured_dir + 'osha_accident_by_year_plot.png')\n",
    "plot_accident_by_year(df, save=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_accident_by_year(df, save=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def plot_accident_by_month(df, norm=False, save=False):\n",
    "    xtab = pd.crosstab(df.event_month, df.Hospitalized)\n",
    "#     print(xtab)\n",
    "    xtab.columns = ['Not Hospitalized', 'Hospitalized']\n",
    "    xtab.columns = ['0','1']\n",
    "    xtab.columns.name = 'Hospitalized Outcome'\n",
    "    ylabel = 'Hospitalized Count'\n",
    "    if norm:\n",
    "        xtab = xtab.div(xtab.sum(axis=1).astype('float'), axis=0)\n",
    "        ylabel = 'Hospitalized Percent'\n",
    "    fig, axes = plt.subplots(1, 1, figsize=(10, 5))\n",
    "    xtab.plot(kind='bar', ax=axes, stacked=True, alpha=0.5, sort_columns=True)\n",
    "    axes.set_title('OSHA Accident Investigations')\n",
    "    axes.set_ylabel(ylabel)\n",
    "    axes.set_xlabel('Event Month')\n",
    "#     if save:\n",
    "#         plt.savefig(structured_dir + 'osha_accident_by_year_plot.png')\n",
    "plot_accident_by_month(df, save=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_accident_by_month(df, norm=False, save=False):\n",
    "    xtab = pd.crosstab(df.event_month, df.Amputation)\n",
    "    xtab.columns = ['Not Fatal', 'Fatal']\n",
    "    xtab.columns = ['0','1']\n",
    "    xtab.columns.name = 'Amputation Outcome'\n",
    "    ylabel = 'Amputation Count'\n",
    "    if norm:\n",
    "        xtab = xtab.div(xtab.sum(axis=1).astype('float'), axis=0)\n",
    "        ylabel = 'Amputation Percent'\n",
    "    fig, axes = plt.subplots(1, 1, figsize=(10, 5))\n",
    "    xtab.plot(kind='bar', ax=axes, stacked=True, alpha=0.5, sort_columns=True)\n",
    "    axes.set_title('OSHA Accident Investigations')\n",
    "    axes.set_ylabel(ylabel)\n",
    "    axes.set_xlabel('Event Month')\n",
    "#     if save:\n",
    "#         plt.savefig(structured_dir + 'osha_accident_by_year_plot.png')\n",
    "plot_accident_by_month(df, save=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DAta scrapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df= df[df['event_year'] == 2015]\n",
    "# df=df.iloc[:100]\n",
    "# df=df[:9853]\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(df[\"Part of Body Title\"]):\n",
    "#     df[\"Part of Body Title\"][i]=df[\"Part of Body Title\"][i].lower()\n",
    "tit_ind = df[df['Part of Body Title']=='Nonclassifiable' ].index\n",
    "len(tit_ind)\n",
    "df.drop(tit_ind , inplace=True)\n",
    "df[\"Part of Body Title\"]=df[\"Part of Body Title\"].str.replace(r'[^\\w\\s]+', '')\n",
    "df[\"Part of Body Title\"]=df[\"Part of Body Title\"].str.replace('unspecified', '')\n",
    "df[\"Part of Body Title\"] = df[\"Part of Body Title\"].apply(clean_text)\n",
    "\n",
    "df[\"EventTitle\"]=df[\"EventTitle\"].str.replace(r'[^\\w\\s]+', '')\n",
    "df[\"EventTitle\"]=df[\"Part of Body Title\"].str.replace('unspecified', '')\n",
    "df[\"EventTitle\"] = df[\"EventTitle\"].apply(clean_text)\n",
    "\n",
    "df[\"NatureTitle\"]=df[\"NatureTitle\"].str.replace(r'[^\\w\\s]+', '')\n",
    "df[\"NatureTitle\"]=df[\"NatureTitle\"].str.replace('unspecified', '')\n",
    "df[\"NatureTitle\"] = df[\"NatureTitle\"].apply(clean_text)\n",
    "\n",
    "df[\"SourceTitle\"]=df[\"SourceTitle\"].str.replace(r'[^\\w\\s]+', '')\n",
    "df[\"SourceTitle\"]=df[\"Part of Body Title\"].str.replace('unspecified', '')\n",
    "df[\"SourceTitle\"] = df[\"SourceTitle\"].apply(clean_text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df)\n",
    "x=set(df['NatureTitle'].values)\n",
    "len(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords') # run this one time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "Let us pre-process the data\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "# function to remove stopwords\n",
    "def remove_stopwords(text):\n",
    "    textArr = text.split(' ')\n",
    "    rem_text = \" \".join([i for i in textArr if i not in stop_words])\n",
    "    return rem_text\n",
    "\n",
    "# remove stopwords from the text\n",
    "df['Part of Body Title']=df['Part of Body Title'].apply(remove_stopwords)\n",
    "df['EventTitle']=df['EventTitle'].apply(remove_stopwords)\n",
    "df['NatureTitle']=df['NatureTitle'].apply(remove_stopwords)\n",
    "df['SourceTitle']=df['SourceTitle'].apply(remove_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python3 -m spacy download en_core_web_md "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_md', disable=['parser', 'ner'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatization(texts,allowed_postags=['NOUN', 'ADJ']): \n",
    "       output = []\n",
    "       for sent in texts:\n",
    "             doc = nlp(sent) \n",
    "             output.append([token.lemma_ for token in doc if token.pos_ in allowed_postags ])\n",
    "       return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "text_list=df['Part of Body Title'].tolist()\n",
    "print(text_list[1])\n",
    "tokenized_reviews = lemmatization(text_list)\n",
    "\n",
    "# print(tokenized_reviews[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_text=tokenized_reviews\n",
    "print(tokenized_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "Create vocabulary dictionary and document term matrix\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = corpora.Dictionary(tokenized_text)\n",
    "doc_term_matrix = [dictionary.doc2bow(rev) for rev in tokenized_reviews]\n",
    "print(len(dictionary))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Part of Body Title'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['NatureTitle'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['SourceTitle'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['EventTitle'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the object for LDA model using gensim library\n",
    "LDA = gensim.models.ldamodel.LdaModel\n",
    "\n",
    "# Build LDA model\n",
    "lda_model = LDA(corpus=doc_term_matrix, id2word=dictionary, num_topics=3, random_state=100,\n",
    "                chunksize=1000, passes=50,iterations=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model.print_topics(num_words=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x= [lda_model.get_document_topics(item) for item in doc_term_matrix]\n",
    "x_f=pd.DataFrame(x)\n",
    "z=[]\n",
    "for i in x:\n",
    "    y=[]\n",
    "    for j in i:\n",
    "#         print(j[1])\n",
    "        y.append(j[1])\n",
    "    z.append(y)\n",
    "topics=[]\n",
    "for i in z:\n",
    "    topics.append(i.index(max(i))+1)\n",
    "# topics\n",
    "df['topics_parts']=topics\n",
    "df\n",
    "\n",
    "# x.print_topics(num_words=33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Visualize the topics\n",
    "#https://github.com/bmabey/pyLDAvis\n",
    "#https://speakerdeck.com/bmabey/visualizing-topic-models\n",
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim_models.prepare(lda_model, doc_term_matrix, dictionary)\n",
    "vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_term_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_list=df['NatureTitle'].tolist()\n",
    "print(text_list[1])\n",
    "tokenized_reviews = lemmatization(text_list)\n",
    "tokenized_text=tokenized_reviews\n",
    "print(tokenized_text)\n",
    "\n",
    "dictionary = corpora.Dictionary(tokenized_text)\n",
    "doc_term_matrix = [dictionary.doc2bow(rev) for rev in tokenized_reviews]\n",
    "\n",
    "LDA = gensim.models.ldamodel.LdaModel\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build LDA model\n",
    "lda_model = LDA(corpus=doc_term_matrix, id2word=dictionary, num_topics=3, random_state=100,\n",
    "                chunksize=1000, passes=50,iterations=100)\n",
    "\n",
    "lda_model.print_topics()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the topics\n",
    "#https://github.com/bmabey/pyLDAvis\n",
    "#https://speakerdeck.com/bmabey/visualizing-topic-models\n",
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim_models.prepare(lda_model, doc_term_matrix, dictionary)\n",
    "vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x= [lda_model.get_document_topics(item) for item in doc_term_matrix]\n",
    "x_f=pd.DataFrame(x)\n",
    "z=[]\n",
    "for i in x:\n",
    "    y=[]\n",
    "    for j in i:\n",
    "#         print(j[1])\n",
    "        y.append(j[1])\n",
    "    z.append(y)\n",
    "topics=[]\n",
    "for i in z:\n",
    "    topics.append(i.index(max(i))+1)\n",
    "# topics\n",
    "df['topics_nature']=topics\n",
    "df\n",
    "\n",
    "# x.print_topics(num_words=33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_list=df['SourceTitle'].tolist()\n",
    "print(text_list[1])\n",
    "tokenized_reviews = lemmatization(text_list)\n",
    "tokenized_text=tokenized_reviews\n",
    "print(tokenized_text)\n",
    "\n",
    "dictionary = corpora.Dictionary(tokenized_text)\n",
    "doc_term_matrix = [dictionary.doc2bow(rev) for rev in tokenized_reviews]\n",
    "\n",
    "LDA = gensim.models.ldamodel.LdaModel\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = LDA(corpus=doc_term_matrix, id2word=dictionary, num_topics=3, random_state=100,\n",
    "                chunksize=1000, passes=50,iterations=100)\n",
    "\n",
    "lda_model.print_topics()\n",
    "# type(lda_model.print_topics())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the topics\n",
    "#https://github.com/bmabey/pyLDAvis\n",
    "#https://speakerdeck.com/bmabey/visualizing-topic-models\n",
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim_models.prepare(lda_model, doc_term_matrix, dictionary)\n",
    "vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x= [lda_model.get_document_topics(item) for item in doc_term_matrix]\n",
    "x_f=pd.DataFrame(x)\n",
    "z=[]\n",
    "for i in x:\n",
    "    y=[]\n",
    "    for j in i:\n",
    "#         print(j[1])\n",
    "        y.append(j[1])\n",
    "    z.append(y)\n",
    "topics=[]\n",
    "for i in z:\n",
    "    topics.append(i.index(max(i))+1)\n",
    "# topics\n",
    "df['topics_source']=topics\n",
    "df\n",
    "\n",
    "# x.print_topics(num_words=33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_list=df['EventTitle'].tolist()\n",
    "print(text_list[1])\n",
    "tokenized_reviews = lemmatization(text_list)\n",
    "tokenized_text=tokenized_reviews\n",
    "print(tokenized_text)\n",
    "\n",
    "dictionary = corpora.Dictionary(tokenized_text)\n",
    "doc_term_matrix = [dictionary.doc2bow(rev) for rev in tokenized_reviews]\n",
    "\n",
    "LDA = gensim.models.ldamodel.LdaModel\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = LDA(corpus=doc_term_matrix, id2word=dictionary, num_topics=4, random_state=100,\n",
    "                chunksize=1000, passes=50,iterations=100)\n",
    "\n",
    "lda_model.print_topics()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the topics\n",
    "#https://github.com/bmabey/pyLDAvis\n",
    "#https://speakerdeck.com/bmabey/visualizing-topic-models\n",
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim_models.prepare(lda_model, doc_term_matrix, dictionary)\n",
    "vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x= [lda_model.get_document_topics(item) for item in doc_term_matrix]\n",
    "x_f=pd.DataFrame(x)\n",
    "z=[]\n",
    "for i in x:\n",
    "    y=[]\n",
    "    for j in i:\n",
    "#         print(j[1])\n",
    "        y.append(j[1])\n",
    "    z.append(y)\n",
    "topics=[]\n",
    "for i in z:\n",
    "    topics.append(i.index(max(i))+1)\n",
    "# topics\n",
    "df['topics_event']=topics\n",
    "df\n",
    "\n",
    "# x.print_topics(num_words=33)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ghxvuqxvuwvyv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.compose import ColumnTransformer      #it \n",
    "# from sklearn.preprocessing import OneHotEncoder   #ifor catogery types , it coverts into binary \n",
    "# ct=ColumnTransformer(transformers=[('encoder',OneHotEncoder())],remainder='passthrough')\n",
    "# #ColTrans(transformers[(kind if trsformer,type of encoding,columns on which we apply)],remainder=).  ->transformers for which we want to apply\n",
    "# #                                                                                                      remainders.  for which we dont eant to apply\n",
    "#     #since ct is both type, and next ct.fittrans rdiesnt return as numpy\n",
    "\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y=pd.DataFrame()\n",
    "y['Amputation']=df['Amputation']\n",
    "y['Hospitalized']=df['Hospitalized']\n",
    "y\n",
    "\n",
    "# z=df['Amputation'].values\n",
    "# z=z.reshape(len(z),1)\n",
    "# y=df['Hospitalized'].values\n",
    "# y=y.reshape(len(y),1)\n",
    "\n",
    "# y=np.append(y,z,axis=1)\n",
    "# # y=y[:1000]\n",
    "# y.shape\n",
    "# y=pd.DataFrame(y,columns=['Hospitalized','Amputation'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x=df[:1000]\n",
    "x=pd.DataFrame()\n",
    "x['Amputation']=df['Amputation']\n",
    "x['Hospitalized']=df['Hospitalized']\n",
    "x['topics_parts']=df['topics_parts']\n",
    "x['topics_event']=df['topics_event']\n",
    "x['topics_source']=df['topics_source']\n",
    "x['topics_nature']=df['topics_nature']\n",
    "\n",
    "# x=x.drop(['IDUPA', 'EventDate','Nature','Part of Body','Event','Source','Hospitalized','Amputation','time_stamp','event_month','event_year','event_weekday'], axis=1)\n",
    "\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=x[x['topics_source']==1]\n",
    "len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=x[x['topics_source']==2]\n",
    "len((y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=x[x['topics_source']==3]\n",
    "len((y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=x[x['topics_event']==4]\n",
    "len((y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ohe = pd.get_dummies(data=x, columns=['topics_parts','topics_event','topics_source','topics_nature'])\n",
    "z=pd.DataFrame(ohe)\n",
    "# z=z.iloc[:,:100]\n",
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_=y.iloc[:,:1]\n",
    "\n",
    "# z[\"Hospitalized\"] = z[\"Hospitalized\"].astype(int)\n",
    "# z[\"Amputation\"] = z[\"Amputation\"].astype(int)\n",
    "\n",
    "\n",
    "z[\"Hospitalized\"] = pd.to_numeric(z[\"Hospitalized\"])\n",
    "z[\"Amputation\"] = pd.to_numeric(z[\"Amputation\"])\n",
    "\n",
    "\n",
    "# qf=pd.concat([z,y], axis=1, join='inner')\n",
    "# qf.dtypes\n",
    "# qf['NatureTitle_abdominal pain'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z['Amputation'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qf=z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train , test=train_test_split(qf,test_size=0.3,random_state=42)\n",
    "# # q=q.transpose()\n",
    "\n",
    "# train_X, test_X, train_y, test_y = train_test_split(q, y_, train_size = 0.3, random_state = 0)\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train))\n",
    "print(len(train.iloc[0]))\n",
    "train_X=train.iloc[:,2:]\n",
    "train_y=train.iloc[:,0]\n",
    "test_X=test.iloc[:,2:]\n",
    "test_y=test.iloc[:,0]\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "sc = MinMaxScaler()\n",
    "\n",
    "\n",
    "# scaler = MinMaxScaler()\n",
    "# model=scaler.fit(data)\n",
    "# scaled_data=model.transform(data)\n",
    "\n",
    "train_X = pd.DataFrame(sc.fit_transform(train_X))\n",
    "test_X = pd.DataFrame(sc.transform(test_X))\n",
    "\n",
    "# train_X = scaler.fit_transform(columns=train_X.columns)\n",
    "# test_X = scaler.fit_transform(columns=test_X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.isnan(train_y.any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "sm = SMOTE(random_state = 42)\n",
    "train_X,train_y  = sm.fit_resample(train_X, train_y)\n",
    "test_X,test_y  = sm.fit_resample(test_X, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "# # import seaborn as sns\n",
    " \n",
    "# std_scaler = StandardScaler()\n",
    " \n",
    "# train_X = std_scaler.fit_transform(train_X)\n",
    "# df_scaled = pd.DataFrame(df_scaled, columns=train_X.columns)\n",
    " \n",
    "# print(\"Scaled Dataset Using StandardScaler\")\n",
    "# X_train=df_scaled\n",
    "\n",
    "\n",
    "# \n",
    "# train_X=(train_X-train_X.mean())/train_X.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_y.columns=['Hospitalized']\n",
    "test_y\n",
    "# print(test_y.Hospitalized.value_counts())\n",
    "# print()\n",
    "print(type(test_y))\n",
    "# print(test_y.Amputation.value_counts())\n",
    "# y.info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=LogisticRegression()\n",
    "model.fit(train_X,train_y)\n",
    "model_pred=model.predict(test_X)\n",
    "print(\"Accuarcy=\",accuracy_score(model_pred,test_y))\n",
    "print('class report is')\n",
    "print(classification_report(model_pred,test_y))\n",
    "print(roc_auc_score(test_y, model_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix , classification_report\n",
    "def print_confusion_matrix(confusion_matrix, class_names, figsize = (10,7), fontsize=14):\n",
    "    df_cm = pd.DataFrame(\n",
    "        confusion_matrix, index=class_names, columns=class_names, \n",
    "    )\n",
    "    fig = plt.figure(figsize=figsize)\n",
    "    try:\n",
    "        heatmap = sns.heatmap(df_cm, annot=True, fmt=\"d\")\n",
    "    except ValueError:\n",
    "        raise ValueError(\"Confusion matrix values must be integers.\")\n",
    "    heatmap.yaxis.set_ticklabels(heatmap.yaxis.get_ticklabels(), rotation=0, ha='right', fontsize=fontsize)\n",
    "    heatmap.xaxis.set_ticklabels(heatmap.xaxis.get_ticklabels(), rotation=45, ha='right', fontsize=fontsize)\n",
    "    plt.ylabel('Truth')\n",
    "    plt.xlabel('Prediction')\n",
    "cm = confusion_matrix(model_pred, test_y)\n",
    "print_confusion_matrix(cm,['is','not'])\n",
    "accuracy_score(test_y, model_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=SVC()\n",
    "model.fit(train_X,train_y)\n",
    "model_pred=model.predict(test_X)\n",
    "print(\"Accuracy=\",accuracy_score(model_pred,test_y))\n",
    "print('classf report is')\n",
    "print(classification_report(model_pred,test_y))\n",
    "print(roc_auc_score(test_y, model_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model=KNeighborsClassifier()\n",
    "model.fit(train_X,train_y)\n",
    "model_pred=model.predict(test_X)\n",
    "print(\"Accuracy\",accuracy_score(model_pred,test_y))\n",
    "print('classf report is')\n",
    "print(classification_report(model_pred,test_y))\n",
    "print(roc_auc_score(test_y, model_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a_index=list(range(1,11))\n",
    "# a=pd.Series()\n",
    "# x=[1,2,3,4,5,6,7,8,9,10]\n",
    "# for i in list(range(1,11)):\n",
    "#     model=KNeighborsClassifier(n_neighbors=i) \n",
    "#     model.fit(train_X,train_y)\n",
    "#     prediction=model.predict(test_X)\n",
    "#     a=a.append(pd.Series(accuracy_score(prediction,test_y)))\n",
    "# plt.plot(a_index, a)\n",
    "# plt.xticks(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod=MultinomialNB()\n",
    "mod.fit(train_X,train_y) \n",
    "model_pred=mod.predict(test_X) \n",
    "print('The accuracy is:',accuracy_score(model_pred,test_y))\n",
    "print(\"The classification report is\")\n",
    "print(classification_report(model_pred,test_y))\n",
    "print(roc_auc_score(test_y, model_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = RandomForestClassifier(class_weight='balanced')\n",
    "model = RandomForestClassifier()\n",
    "model.fit(train_X,train_y) \n",
    "prediction=model.predict(test_X) \n",
    "print('The accuracy =',accuracy_score(prediction,test_y))\n",
    "print(\"The classification report is\")\n",
    "print(classification_report(prediction,test_y))\n",
    "print(roc_auc_score(test_y, prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from matplotlib import pyplot as plt\n",
    "# from sklearn.metrics import confusion_matrix , classification_report\n",
    "# def print_confusion_matrix(confusion_matrix, class_names, figsize = (10,7), fontsize=14):\n",
    "#     df_cm = pd.DataFrame(\n",
    "#         confusion_matrix, index=class_names, columns=class_names, \n",
    "#     )\n",
    "#     fig = plt.figure(figsize=figsize)\n",
    "#     try:\n",
    "#         heatmap = sns.heatmap(df_cm, annot=True, fmt=\"d\")\n",
    "#     except ValueError:\n",
    "#         raise ValueError(\"Confusion matrix values must be integers.\")\n",
    "#     heatmap.yaxis.set_ticklabels(heatmap.yaxis.get_ticklabels(), rotation=0, ha='right', fontsize=fontsize)\n",
    "#     heatmap.xaxis.set_ticklabels(heatmap.xaxis.get_ticklabels(), rotation=45, ha='right', fontsize=fontsize)\n",
    "#     plt.ylabel('Truth')\n",
    "#     plt.xlabel('Prediction')\n",
    "# cm = confusion_matrix(prediction, test_y)\n",
    "# print_confusion_matrix(cm,['is','not'])\n",
    "# # accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DecisionTreeClassifier()\n",
    "model.fit(train_X,train_y) \n",
    "prediction=model.predict(test_X) \n",
    "print('The accuracy is:',accuracy_score(prediction,test_y))\n",
    "print(\"The classification report is\")\n",
    "print(classification_report(prediction,test_y))\n",
    "print(roc_auc_score(test_y, prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = XGBClassifier()\n",
    "model.fit(train_X,train_y) \n",
    "prediction=model.predict(test_X) \n",
    "print('The accuracy is:',accuracy_score(prediction,test_y))\n",
    "print(\"The classification report is\")\n",
    "print(classification_report(prediction,test_y))\n",
    "print(roc_auc_score(test_y, prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X=train.iloc[:,2:]\n",
    "train_y=train.iloc[:,1]\n",
    "test_X=test.iloc[:,2:]\n",
    "test_y=test.iloc[:,1]\n",
    "train_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_y.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hospitalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train))\n",
    "print(len(train.iloc[0]))\n",
    "train_X=train.iloc[:,2:]\n",
    "train_y=train.iloc[:,1]\n",
    "test_X=test.iloc[:,2:]\n",
    "test_y=test.iloc[:,1]\n",
    "\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# sc = StandardScaler()\n",
    "\n",
    "# train_X = pd.DataFrame(sc.fit_transform(X_train))\n",
    "# test_X = pd.DataFrame(sc.transform(X_test))\n",
    "\n",
    "print(train_X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "sm = SMOTE(random_state = 2)\n",
    "train_X,train_y  = sm.fit_resample(train_X, train_y)\n",
    "test_X,test_y  = sm.fit_resample(test_X, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_y.columns=['Hospitalized']\n",
    "test_y\n",
    "# print(test_y.Hospitalized.value_counts())\n",
    "# print()\n",
    "print(type(test_y))\n",
    "# print(test_y.Amputation.value_counts())\n",
    "# y.info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score,roc_auc_score\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=LogisticRegression()\n",
    "model.fit(train_X,train_y)\n",
    "model_pred=model.predict(test_X)\n",
    "print(\"Accuarcy=\",accuracy_score(model_pred,test_y))\n",
    "print('class report is')\n",
    "print(classification_report(model_pred,test_y))\n",
    "print(precision_score(model_pred,test_y))\n",
    "print(recall_score(test_y, model_pred))\n",
    "print(roc_auc_score(test_y, model_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix , classification_report\n",
    "def print_confusion_matrix(confusion_matrix, class_names, figsize = (10,7), fontsize=14):\n",
    "    df_cm = pd.DataFrame(\n",
    "        confusion_matrix, index=class_names, columns=class_names, \n",
    "    )\n",
    "    fig = plt.figure(figsize=figsize)\n",
    "    try:\n",
    "        heatmap = sns.heatmap(df_cm, annot=True, fmt=\"d\")\n",
    "    except ValueError:\n",
    "        raise ValueError(\"Confusion matrix values must be integers.\")\n",
    "    heatmap.yaxis.set_ticklabels(heatmap.yaxis.get_ticklabels(), rotation=0, ha='right', fontsize=fontsize)\n",
    "    heatmap.xaxis.set_ticklabels(heatmap.xaxis.get_ticklabels(), rotation=45, ha='right', fontsize=fontsize)\n",
    "    plt.ylabel('Truth')\n",
    "    plt.xlabel('Prediction')\n",
    "cm = confusion_matrix(model_pred, test_y)\n",
    "print_confusion_matrix(cm,['is','not'])\n",
    "accuracy_score(test_y, model_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=SVC()\n",
    "model.fit(train_X,train_y)\n",
    "model_pred=model.predict(test_X)\n",
    "print(\"Accuracy=\",accuracy_score(model_pred,test_y))\n",
    "print('classf report is')\n",
    "print(classification_report(model_pred,test_y))\n",
    "print(precision_score(model_pred,test_y))\n",
    "print(recall_score(test_y, model_pred))\n",
    "print(roc_auc_score(test_y, model_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix , classification_report\n",
    "def print_confusion_matrix(confusion_matrix, class_names, figsize = (10,7), fontsize=14):\n",
    "    df_cm = pd.DataFrame(\n",
    "        confusion_matrix, index=class_names, columns=class_names, \n",
    "    )\n",
    "    fig = plt.figure(figsize=figsize)\n",
    "    try:\n",
    "        heatmap = sns.heatmap(df_cm, annot=True, fmt=\"d\")\n",
    "    except ValueError:\n",
    "        raise ValueError(\"Confusion matrix values must be integers.\")\n",
    "    heatmap.yaxis.set_ticklabels(heatmap.yaxis.get_ticklabels(), rotation=0, ha='right', fontsize=fontsize)\n",
    "    heatmap.xaxis.set_ticklabels(heatmap.xaxis.get_ticklabels(), rotation=45, ha='right', fontsize=fontsize)\n",
    "    plt.ylabel('Truth')\n",
    "    plt.xlabel('Prediction')\n",
    "cm = confusion_matrix(model_pred, test_y)\n",
    "print_confusion_matrix(cm,['is','not'])\n",
    "accuracy_score(test_y, model_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=KNeighborsClassifier()\n",
    "model.fit(train_X,train_y)\n",
    "model_pred=model.predict(test_X)\n",
    "print(\"Accuracy\",accuracy_score(model_pred,test_y))\n",
    "print('classf report is')\n",
    "print(classification_report(model_pred,test_y))\n",
    "print(roc_auc_score(model_pred,test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix , classification_report\n",
    "def print_confusion_matrix(confusion_matrix, class_names, figsize = (10,7), fontsize=14):\n",
    "    df_cm = pd.DataFrame(\n",
    "        confusion_matrix, index=class_names, columns=class_names, \n",
    "    )\n",
    "    fig = plt.figure(figsize=figsize)\n",
    "    try:\n",
    "        heatmap = sns.heatmap(df_cm, annot=True, fmt=\"d\")\n",
    "    except ValueError:\n",
    "        raise ValueError(\"Confusion matrix values must be integers.\")\n",
    "    heatmap.yaxis.set_ticklabels(heatmap.yaxis.get_ticklabels(), rotation=0, ha='right', fontsize=fontsize)\n",
    "    heatmap.xaxis.set_ticklabels(heatmap.xaxis.get_ticklabels(), rotation=45, ha='right', fontsize=fontsize)\n",
    "    plt.ylabel('Truth')\n",
    "    plt.xlabel('Prediction')\n",
    "cm = confusion_matrix(model_pred, test_y)\n",
    "print_confusion_matrix(cm,['is','not'])\n",
    "accuracy_score(test_y, model_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a_index=list(range(1,11))\n",
    "# a=pd.Series()\n",
    "# x=[1,2,3,4,5,6,7,8,9,10]\n",
    "# for i in list(range(1,11)):\n",
    "#     model=KNeighborsClassifier(n_neighbors=i) \n",
    "#     model.fit(train_X,train_y)\n",
    "#     prediction=model.predict(test_X)\n",
    "#     a=a.append(pd.Series(accuracy_score(prediction,test_y)))\n",
    "# plt.plot(a_index, a)\n",
    "# plt.xticks(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod=MultinomialNB()\n",
    "mod.fit(train_X,train_y) \n",
    "model_pred=mod.predict(test_X) \n",
    "print('The accuracy is:',accuracy_score(model_pred,test_y))\n",
    "print(\"The classification report is\")\n",
    "print(classification_report(model_pred,test_y))\n",
    "print(roc_auc_score(model_pred,test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix , classification_report\n",
    "def print_confusion_matrix(confusion_matrix, class_names, figsize = (10,7), fontsize=14):\n",
    "    df_cm = pd.DataFrame(\n",
    "        confusion_matrix, index=class_names, columns=class_names, \n",
    "    )\n",
    "    fig = plt.figure(figsize=figsize)\n",
    "    try:\n",
    "        heatmap = sns.heatmap(df_cm, annot=True, fmt=\"d\")\n",
    "    except ValueError:\n",
    "        raise ValueError(\"Confusion matrix values must be integers.\")\n",
    "    heatmap.yaxis.set_ticklabels(heatmap.yaxis.get_ticklabels(), rotation=0, ha='right', fontsize=fontsize)\n",
    "    heatmap.xaxis.set_ticklabels(heatmap.xaxis.get_ticklabels(), rotation=45, ha='right', fontsize=fontsize)\n",
    "    plt.ylabel('Truth')\n",
    "    plt.xlabel('Prediction')\n",
    "cm = confusion_matrix(model_pred, test_y)\n",
    "print_confusion_matrix(cm,['is','not'])\n",
    "accuracy_score(test_y, model_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = RandomForestClassifier(class_weight='balanced')\n",
    "model = RandomForestClassifier()\n",
    "model.fit(train_X,train_y) \n",
    "prediction=model.predict(test_X) \n",
    "print('The accuracy =',accuracy_score(prediction,test_y))\n",
    "print(\"The classification report is\")\n",
    "print(classification_report(prediction,test_y))\n",
    "print(roc_auc_score(prediction,test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix , classification_report\n",
    "def print_confusion_matrix(confusion_matrix, class_names, figsize = (10,7), fontsize=14):\n",
    "    df_cm = pd.DataFrame(\n",
    "        confusion_matrix, index=class_names, columns=class_names, \n",
    "    )\n",
    "    fig = plt.figure(figsize=figsize)\n",
    "    try:\n",
    "        heatmap = sns.heatmap(df_cm, annot=True, fmt=\"d\")\n",
    "    except ValueError:\n",
    "        raise ValueError(\"Confusion matrix values must be integers.\")\n",
    "    heatmap.yaxis.set_ticklabels(heatmap.yaxis.get_ticklabels(), rotation=0, ha='right', fontsize=fontsize)\n",
    "    heatmap.xaxis.set_ticklabels(heatmap.xaxis.get_ticklabels(), rotation=45, ha='right', fontsize=fontsize)\n",
    "    plt.ylabel('Truth')\n",
    "    plt.xlabel('Prediction')\n",
    "cm = confusion_matrix(model_pred, test_y)\n",
    "print_confusion_matrix(cm,['is','not'])\n",
    "accuracy_score(prediction, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DecisionTreeClassifier()\n",
    "model.fit(train_X,train_y) \n",
    "prediction=model.predict(test_X) \n",
    "print('The accuracy is:',accuracy_score(prediction,test_y))\n",
    "print(\"The classification report is\")\n",
    "print(classification_report(prediction,test_y))\n",
    "print(roc_auc_score(prediction,test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix , classification_report\n",
    "def print_confusion_matrix(confusion_matrix, class_names, figsize = (10,7), fontsize=14):\n",
    "    df_cm = pd.DataFrame(\n",
    "        confusion_matrix, index=class_names, columns=class_names, \n",
    "    )\n",
    "    fig = plt.figure(figsize=figsize)\n",
    "    try:\n",
    "        heatmap = sns.heatmap(df_cm, annot=True, fmt=\"d\")\n",
    "    except ValueError:\n",
    "        raise ValueError(\"Confusion matrix values must be integers.\")\n",
    "    heatmap.yaxis.set_ticklabels(heatmap.yaxis.get_ticklabels(), rotation=0, ha='right', fontsize=fontsize)\n",
    "    heatmap.xaxis.set_ticklabels(heatmap.xaxis.get_ticklabels(), rotation=45, ha='right', fontsize=fontsize)\n",
    "    plt.ylabel('Truth')\n",
    "    plt.xlabel('Prediction')\n",
    "cm = confusion_matrix(model_pred, test_y)\n",
    "print_confusion_matrix(cm,['is','not'])\n",
    "accuracy_score(prediction, test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = XGBClassifier()\n",
    "model.fit(train_X,train_y) \n",
    "prediction=model.predict(test_X) \n",
    "print('The accuracy is:',accuracy_score(prediction,test_y))\n",
    "print(\"The classification report is\")\n",
    "print(classification_report(prediction,test_y))\n",
    "print(roc_auc_score(prediction,test_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print('\\nPerplexity: ', lda_model.log_perplexity(doc_term_matrix,total_docs=10000))  # a measure of how good the model is. lower the better.\n",
    "\n",
    "# Compute Coherence Score\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=tokenized_reviews, dictionary=dictionary , coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "Method to find optimal number of topics\n",
    "Code from:https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/#14computemodelperplexityandcoherencescore\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def compute_coherence_values(dictionary, corpus, texts, limit, start=2, step=3):\n",
    "    \"\"\"\n",
    "    Compute c_v coherence for various number of topics\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    dictionary : Gensim dictionary\n",
    "    corpus : Gensim corpus\n",
    "    texts : List of input texts\n",
    "    limit : Max num of topics\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    model_list : List of LDA topic models\n",
    "    coherence_values : Coherence values corresponding to the LDA model with respective number of topics\n",
    "    \"\"\"\n",
    "    coherence_values = []\n",
    "    model_list = []\n",
    "    for num_topics in range(start, limit, step):\n",
    "        model = gensim.models.ldamodel.LdaModel(corpus=corpus, num_topics=num_topics, id2word=dictionary)\n",
    "        model_list.append(model)\n",
    "        coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "        coherence_values.append(coherencemodel.get_coherence())\n",
    "\n",
    "    return model_list, coherence_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list, coherence_values = compute_coherence_values(dictionary=dictionary, corpus=doc_term_matrix, texts=tokenized_reviews, start=2, limit=50, step=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show graph\n",
    "limit=50; start=2; step=1;\n",
    "x = range(start, limit, step)\n",
    "plt.plot(x, coherence_values)\n",
    "plt.xlabel(\"Num Topics\")\n",
    "plt.ylabel(\"Coherence score\")\n",
    "plt.legend((\"coherence_values\"), loc='best')\n",
    "plt.show()# Print the coherence scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the coherence scores\n",
    "for m, cv in zip(x, coherence_values):\n",
    "    print(\"Num Topics =\", m, \" has Coherence Value of\", round(cv, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the model and print the topics\n",
    "optimal_model = model_list[7]\n",
    "model_topics = optimal_model.show_topics(formatted=False)\n",
    "optimal_model.print_topics(num_words=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the topics\n",
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim.prepare(optimal_model, doc_term_matrix, dictionary)\n",
    "vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x='2'\n",
    "type(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
